{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_text import spacy_txt\n",
    "import pandas as pd\n",
    "from snapy import MinHash, LSH\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id = 'doc11.txt_sentence_14_noun_32'\n",
    "#doc_index=int(re.sub('(?<=sentence).+','',word_id).replace('sentence',''))\n",
    "#index = re.sub('.+(?=sentence)','',word_id).replace('sentence','')\n",
    "#word_index = int(re.sub('.+(?=_)','',index).replace('_',''))\n",
    "#sent_index= int(re.sub('(?<=_).+','',index).replace('_',''))\n",
    "\n",
    "doc_index = re.findall('.+(?=_sentence)',word_id)[0]\n",
    "sent_index= int(re.findall('(?<=sentence_).+(?=_noun)',word_id)[0])\n",
    "noun_index=int(re.findall('(?<=noun_).+',word_id)[0])\n",
    "noun_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DEA\"\n",
    "s = spacy_txt()\n",
    "doc, e, n= s.read_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_fp = '/teams/DSC180A_FA20_A00/a07opioidoverdoseprevalence/data/MESH.csv'\n",
    "rxnorm_fp = '/teams/DSC180A_FA20_A00/a07opioidoverdoseprevalence/data/RXNORM.csv'\n",
    "rx=pd.read_csv(rxnorm_fp,usecols=['Preferred Label','Semantic type UMLS property'])\n",
    "mesh=pd.read_csv(mesh_fp,usecols=['Preferred Label','Semantic type UMLS property'])\n",
    "term = pd.concat([rx,mesh])\n",
    "term['Preferred Label'] = term['Preferred Label'].str.lower()\n",
    "ontology = dict(zip(term['Preferred Label'],term['Semantic type UMLS property']))\n",
    "terms = list(ontology.keys())\n",
    "#terms = terms[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = sorted([i.strip('(+)-').strip('(),.-{').strip().replace(\"'\",'').replace(\"}\",'') for i in terms])\n",
    "dic={}\n",
    "for i in terms:\n",
    "    c = i[0]\n",
    "    if c in dic:\n",
    "        dic[c]+=1\n",
    "    else:\n",
    "        dic[c]=1\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_text(doc,doc_id):\n",
    "    #Initialize a dictionary\n",
    "    content = dict()       \n",
    "\n",
    "    #Add noun to dictionary with its location as key\n",
    "    for sent_id in doc.sentences:\n",
    "        sent=doc.sentences[sent_id]\n",
    "        for n_id,noun in enumerate(sent.noun):\n",
    "            key=str(doc_id)+'sentence'+str(sent_id)+'_'+str(n_id)\n",
    "            noun = noun.lower()\n",
    "            if len(noun) > 3:\n",
    "                content[key] = noun\n",
    "               \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_search(search_terms):\n",
    "    #Initialize a dictionary\n",
    "    content = dict()\n",
    "    \n",
    "    #Add search terms to dictionary with numbered key\n",
    "    for i in range(len(search_terms)):\n",
    "        if len(search_terms[i]) > 3:\n",
    "            content[i] = search_terms[i].strip().lower()    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = make_content_search(terms)\n",
    "for doc_id,document in enumerate(doc):\n",
    "    content.update(make_content_text(document,doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lsh(content, n_permutations, n_gram):\n",
    "    labels = content.keys()\n",
    "    values = content.values()\n",
    "    #Create MinHash object\n",
    "    minhash = MinHash(values, n_gram=n_gram, permutations=n_permutations, hash_bits=64, seed=3)\n",
    "    #Create LSH model\n",
    "    lsh = LSH(minhash, labels, no_of_bands=5)\n",
    "    return lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations= 50,\n",
    "n_gram= 3\n",
    "%time lsh = create_lsh(content,n_permutations,n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = lsh.edge_list(min_jaccard=0.2,jaccard_weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "similar_ls = []\n",
    "for i in edge_list:\n",
    "    if type(i[1])==int and type(i[0])==str:\n",
    "        similar_ls.append(i)\n",
    "        print(edge_list.index(i))\n",
    "    elif (type(i[0])==int and type(i[1])==str):\n",
    "        similar_ls.append(i)\n",
    "        print(edge_list.index(i))\n",
    "    elif type(i[0])==int and type(i[1])==int:\n",
    "        break\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_entity_type(docs,similar_ls,terms,ontology):\n",
    "    for i in similar_ls:\n",
    "        word_id = i[0]\n",
    "        term_id = i[1]\n",
    "        score = i[2]\n",
    "        #get the entity \n",
    "        word = content[word_id]\n",
    "        #get the sentence and word index of the entity\n",
    "        doc_index=int(re.sub('(?<=sentence).+','',word_id).replace('sentence',''))\n",
    "        index = re.sub('.+(?=sentence)','',word_id).replace('sentence','')\n",
    "        word_index = int(re.sub('.+(?=_)','',index).replace('_',''))\n",
    "        sent_index= int(re.sub('(?<=_).+','',index).replace('_',''))\n",
    "\n",
    "        types = []\n",
    "        scores = []\n",
    "        # get ontology type for entity\n",
    "        onto = ontology[terms[term_id]]\n",
    "        types.append(onto)\n",
    "        scores.append(score)\n",
    "        \n",
    "        #check if the noun is already in ner list\n",
    "        in_ner =False\n",
    "        ner_score =1\n",
    "        doc = docs[doc_index]\n",
    "        sent = doc.sentences[sent_index]\n",
    "        #loop through list of entity dictionary for the sentence\n",
    "        for entity in sent.ner:\n",
    "            current = list(entity.keys())[0]\n",
    "            if word==current.lower():\n",
    "                types.append(entity[current])\n",
    "                scores.append(ner_score)\n",
    "                entity[current] = [types,scores]\n",
    "                in_ner = True\n",
    "                print(doc_index,sent_index)\n",
    "                \n",
    "        if not in_ner:\n",
    "            #store type of entity to its location in the list\n",
    "            sent.ner.append({word:[types,scores]})\n",
    "            print(doc_index,sent_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "update_entity_type(doc,similar_ls,terms,ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentence.ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
